\chapter{Simulations}
\section{Plasma simulations}
%The accuracy by which modern plasma simulation codes are able to model plasma behaviour has made them a crucial part of current plasma physics research. 
Numerical simulations are a crucial part of plasma physics research. The accuracy by which modern plasma simulation codes are able to model plasma behaviour allows us to better understand and improve the functioning of our current experiments, as well as to explore and develop new novel ideas without the technological and financial limitations of real-world experiments. As a result, simulation studies play an increasingly more decisive role in guiding and accelerating current plasma wakefield research. \\
\indent Given that a plasma is, to a good approximation, nothing more than electrons and ions interacting electromagnetically, the response of such a plasma to the propagation of an electron beam or laser pulse could in theory be simulated by solving Maxwell's equations and calculating the Lorentz force law for each electron and ion in the plasma. This approach is however computationally intractable due to vast number of particles present in the simulations we need to perform. We circumvent this computational road block by making use of so-called Particle-In-Cell (PIC) codes. In this chapter we outline the general PIC approach and introduce the plasma physics PIC code EPOCH, which is used throughout this project. We further detail the modifications necessary to allow the hybrid beam dump scheme to be simulated with EPOCH. 



%Simulations because: Cheaper than experiments, more readily available to anyone, simulations allow us to study, understand and exploit these phenomena without the need to repeatedly perform expensive and intricate experiments...Furthermore, by having a simulated rather than physical experiment, one may avoid the uncertainties and noise present in the real world and may therefore investigate and even discover physical phenomena that are too sensitive to be detected in noisy data samples. 

%To take advantage of simulations it is however crucial to know the accuracy by which the simulations model the physical situation and to understand the limitations that this imposes. For instance, as will be shown in section XXX, failing to model the experiment with high enough resolution can lead to phenomena emerging from purely numerical features in the simulations. One must therefore be confident that the results seen in simulations accurately represent the physics at hand, either by comparing the simulations to experimental data or theoretical calculations if available. The non-linear nature of the high-energy plasma wakefield phenomena that we wish to model in this project do not lend themselves easily to analytical treatments. To investigate these phenomena and provide useful results for future experiments we will make extensive use of simulations in this project.\\ 



%In this chapter we introduce the plasma physics PIC code EPOCH, which is used throughout this project, and detail the modifications necessary to allow the hybrid beam dump scheme to be simulated. 

%The behaviour of these macro particles is then calculated and used as a representation of the response of the actual plasma.  where each macroscopic particle carries the total charge and mass of the microscopic particles it represents. 



\section{Particle-in-Cell Codes}
\label{sec:Particle-in-Cell Codes}
The key feature of all PIC codes is to represent a large ensemble of microscopic particles by a smaller ensamble of macroscopic pseudo-particles on a discrete spatial grid \cite{Pukhov2015}. Each of these macroparticles carries the total charge and mass of the collection of electrons or ions that it represents and, importantly, are not point particles but finite regions of space, 'cells', that move as one. The interactions and dynamical behaviour of these macroparticles is then used as a representation of the actual plasma response. This significantly reduces the computational cost. A further simplifying feature of PIC codes is that the EM fields are only calculated on discrete points on a so-called Yee grid whilst the macroparticles move continuously through this grid, ensuring that accurate dynamical behaviour is maintained \cite{Lawrence-Douglas2013}. Hence, once a distribution of moving charged particles has been introduced into this grid the EM fields can be computed by solving Maxwell's equations in discretized time, which in turn govern the subsequent motion on the particles. However, the fact that the discrete space of fields is disjoint from the continuous field of particles means that we need an approach to translate between them. We outline the general method to do this in figure \ref{PIC_loop}, which involves an iterative loop of so-called particle push and field solver algorithms. We start by considering a known spacial distribution of particles and their associated velocities, $\left(\boldsymbol{r}_{n},\boldsymbol{v}_{n}\right)$, at a time $t_n$. In order to compute the the induced EM fields at discrete grid points the charged macroparticles are weighted onto several of the nearest grid points, thus resulting in each grid point having an associated charged distribution and current, $\left(\rho_{n},\boldsymbol{J}_{n}\right)$. The corresponding EM fields, which we label by $n+1$ since these will act to move our particles, are found through Maxwell's equation via a \textit{field solver} algorithm. This is necessitated by the fact that the EM fields at n+1, $\left(\boldsymbol{E}_{n+1},\boldsymbol{B}_{n+1}\right)$, are not only sourced by $\rho_{n}$ and $\boldsymbol{J}_{n}$ but also by the variation of the fields, $\Delta\vec{B}$ and $\Delta\vec{E}$, between $n$ and $n+1$ through Ampere's and Faraday's laws. Hence the fields can not be computed at the same time since, for instance, $\vec{B}_{n+1 }$ is in part induced by $\Delta\vec{E}=\vec{E}_{n+1}-\vec{E}_n$, where $\vec{E}_{n+1}$ is in turn induced by $\Delta\vec{B}=\vec{B}_{n+1}-\vec{B}_n$. This issue is addressed by the \textit{leapfrog} method, whereby particle positions and velocities are calculate at alternating half-way intermediate steps $n+1/2$, but out of step by $\Delta t/2$. This means that we compute $\boldsymbol{r}_{n}$ and $\boldsymbol{v}_{n-1/2}$ simultaneously, followed by $\boldsymbol{r}_{n+1}$ and $\boldsymbol{v}_{n+1/2}$. Although not immediately obvious, this allows us to calculate the EM fields in the following chain: $\left(\vec{E}_n,\vec{B}_n\right) \to (\vec{E}_n,\vec{B}_{n+1/2}) \to (\vec{E}_{n+1},\vec{B}_{n+1/2})  \to  (\vec{E}_{n+1},\vec{B}_{n+1})$. It can further be shown that this method scales as the square of the distance successive grid points \cite{Lawrence-Douglas2013}, and is thus a second-order numerical simulation scheme. Once these fields are known the first step of the loop is effectively done in reverse by weighting the fields off the grid and back onto each individual macroparticle. Then using a similar leapfrog method in a \textit{particle pusher} algorithm the Lorentz force law is used to push the particles to their new positions and velocities, thus completing the iteration step. 
%\begin{figure}
%\centering
%\includegraphics[width=0.5\textwidth]{YeeGrid.pdf}
%\caption{Flowchart showing the general operations conducted during one time iteration of the Particle-In-Cell simulation method. The colours indicate whether the EM fields and particle distributions are calculated at points on the discretized Yee grid (orange) or in the continuous space of particles (blue). The field update and particle push algorithms}
%\label{PIC_loop}
%\end{figure}
\begin{figure}
\centering
\includegraphics[width=\textwidth]{PIC_loop2.pdf}
\caption{Flowchart showing the general operations conducted during one time iteration of the Particle-In-Cell simulation method. The colours indicate whether the EM fields and particle distributions are calculated at points on the discretized Yee grid (orange) or in the continuous space of particles (blue). The field update and particle push algorithms}
\label{PIC_loop}
\end{figure}


\section{EPOCH}
%The simulations presented in this thesis are generated using the open-source plasma physics PIC simulation code \textsc{EPOCH},
The Extensible PIC Open Collaboration project (EPOCH) is an advance relativistic electromagnetic PIC code developed at the University of Warwick \cite{Bennett2015}. EPOCH is now maintained and developed through the Collaborative Computational Project in Plasma Physics (CCP-Plasma), from which access to the code is granted to non-profit research laboratories and Universities \cite{ChrisBRADYKeithBENNETTHolgerSCHMITZ}. 
%Simulations  using EPOCH simply require users to specify the parameters and initial conditions of the simulations without the need to interact with the underlying PIC code.
The underlying code is written in Fortran and allows for simulations to be run on multiple parallel processors via MPI; this enables time-consuming simulations to be run on remote computing clusters. The core PIC code in EPOCH is based upon the field solver and particle push algorithms of the Plasma Simulation Code (PSC) written by H. Ruhl \cite{Ruhl}. This follows closely the standard PIC scehme outline in section \ref{sec:Particle-in-Cell Codes}. The main difference being the use of a more precise leapfrog method and the inclusion of additional functionality to allow for more advanced features such as collisions, ionisation and quantum electrodynamic radiation to be simulated. Furthermore, EPOCH is highly user-friendly; setting up simulations simply requires users to specify the parameters and initial conditions of the simulations without the need to interact with the underlying PIC code. Likewise, analysing and visualising data from a simulations is made easier through file-compatibility with Python, Matlab, IDL and VisIt.
\subsection{Input deck}
Once EPOCH has been downloaded and compiled the so-called input deck is essentially EPOCH's user interface. This is a file in which users specify the details of a simulations and it is this file that gets read by EPOCH and passed onto the core PIC algorithm. The input deck consists of blocks which define parameters for different features of the simulation. \\
\textbf{Explain control block first}, and what the restart does.
This specifies the grid that the simulations is to run on. We then populate this grid with plasma particles.
\textbf{Species block}, with explanation about analytical density distributions for plasma, and specify ppc.\\
The control and species blocks together define the resolution of the simulation. When setting up the resolution of the grid one has to make sure that the grid is sufficiently fine such that the smallest features of our physical system are resolved. This is to ensure that the simulation accurately models the physical system it is meant to represent, to the extent that missing small scale phenomena might alter the large scale outcome of the simulation. A finer grid however requires more macroparticles to fully populate the grid, which inevitably extents the computational time. In addition the time step $\Delta t$ aneeds to be suitably decreased as well. This is because of the so-called Courant-Friedrichs-Lewy (CFL) condition.  Any simulation introduces uncertainties in the final outcome due to the finite resolution. We need to make sure that the uncertainties introduces during each iteration do not build up and grow unbounded. \\

\subsection{Non-analytical bunch initialisation}
The hybrid scheme approach that the endeavour to investigate in this project relies on us being able to simulate a laser pulse propagating in front of a pre-saturated bunch. Previous investigations by our colleagues have shown that this is not a trivial task using the simulation software that have been tried. The issue with epoch in this regard is that everything that is to be included in the simulation need to be specified in the input deck and introduced at the beging of a simulation. Hence, simulations of the active beam dump are entirely possible in EPOCH, since a laser can be initialised in front of a bunch at $t=0$ and driven together. Introducing a laser at a later stage $t>0$ appeared however not to be possible. The majority of the work so far in this project has been concerned with solving this issue. The approach taken was to simulate the passive part first and then export the data of the saturated bunch into a new simulation which includes a laser driver. The structure of the SDF data files output by EPOCH are however not suitable for accessing and retrieving particular sets of data. A workaround was found by using VisIt to read the files and subsequently export only the data related to the electron beam. This data was subsequently read by EPOCH during the compilation stage in a specific file included in EPOCH. This file, usually left empty, allows users to over-ride section of the input file by assigning custom values to each of the macroparticles that are intialised in at the start of a simulation. Through this approach we are now able to define a simulation where a laser pulse is driven ahead of a standard bi-gaussian electron beam. By then over-riding the parameters for the electron beam we can then use the data from a pre-saturated bunch to reposition each macroparticle in the bi-gaussian to its corresponding position relative to the rest of the bunch. By also over-riding momentum parameters we can completely mould the bunch into the one we had exported after a passive beam dump. \\
\\
Real accelerators operate at a range of beam parameters tailored to specific reserach purposes. In order to avoid having to set up week-long simulations for each minor change of parameters it is crucial to understand the theory underlying these plasma wakefield phenomena and to see to what extent the theoretical predictions match the simulations

For any pracitactal implementation of the hybrid scheme to an existing experiment it 
