\chapter{Simulations}
\begin{itemize}
\item Ways to simulate plasmas, different techniques/codes etc.
\end{itemize}
\section{Plasma simulations}
\textbf{Why simulations}\\
Simulations because: Cheaper than experiments, more readily available to anyone, simulations allow us to study, understand and exploit these phenomena without the need to repeatedly perform expensive and intricate experiments...Furthermore, by having a simulated rather than physical experiment, one may avoid the uncertainties and noise present in the real world and may therefore investigate and even discover physical phenomena that are too sensitive to be detected in noisy data samples. To take advantage of simulations it is however crucial to know the accuracy by which the simulations model the physical situation and to understand the limitations that this imposes. For instance, as will be shown in section XXX, failing to model the experiment with high enough resolution can lead to phenomena emerging from purely numerical features in the simulations. One must therefore be confident that the results seen in simulations accurately represent the physics at hand, either by comparing the simulations to experimental data or theoretical calculations if available. The non-linear nature of the high-energy plasma wakefield phenomena that we wish to model in this project do not lend themselves easily to analytical treatments. To investigate these phenomena and provide useful results for future experiments we will make extensive use of simulations in this project.\\ 
\indent Given that a plasma is no more than electrons and ions interacting electromagnetically, the response of such a plasma to the propagation of an electron bunch or laser pulse could in theory be simulated by solving Maxwell's equations for a set of initial conditions. This would involve solving Maxwell's equations at time an initial time $t_0$ and calculating the combined electromagnetic fields acting on each particle in the plasma. Then, by considering each particles velocity, one could calculate the new positions and velocities of all particles for a small time increase $t_0+\Delta t$. Repeating these computation would  lead us to find the approximate plasma response at any arbitrary time $t>t_0$. However, this approach is computationally intractable. Since if we attempt this approach in most plasma simulations. For instance, if we consider that the plasma in a typical plasma wakefield accelerator [Hanahoe] is on the order of centimetres in extent, with a number density $~10^{20} m^{-3}$, we find that we have on the order of $10^{14}$ electrons in the plasma. All these electrons would have to be included in the simulation and stored with their associated 6-dimensional position and velocity data $(x,y,z,v_x,v_y,v_z)$. Each number would be stored as a 32-bit double precision floating point number, yielding the total data size required for the whole plasma simulation on the order of a petabyte ($10^{15}$ bytes).\\
\indent To circumvent this computational road block we make use of so-called Particle-In-Cell (PIC) codes, in which a large collections of physical microscopic particles are represented as smaller collections of macroscopic pseudo-particles on a grid. In this chapter we outline the general PIC approach and introduce the plasma physics PIC code EPOCH, which is used throughout this project. We further detail the modifications necessary to allow the hybrid beam dump scheme to be simulated on EPOCH. 

%In this chapter we introduce the plasma physics PIC code EPOCH, which is used throughout this project, and detail the modifications necessary to allow the hybrid beam dump scheme to be simulated. 

%The behaviour of these macro particles is then calculated and used as a representation of the response of the actual plasma.  where each macroscopic particle carries the total charge and mass of the microscopic particles it represents. 

\section{Particle-in-Cell Codes}
\label{sec:Particle-in-Cell Codes}
Starting from EM fields $\vec{E}_{(n)},\vec{B}_{(n)}$ and charge current $\vec{J}_{(n)}$ present at iteration $n$ [at a specific position, middle of Yee grid?] we obtained the fields at the next time step $n+1$ by computing the resulting fields and currents at an intermediate half-way step $n+1/2$. We do this by first computing the change in the electric field, using Ampere's law, $\Delta \vec{E}_{(n)}$ which we add to our current field such that
\begin{equation}
\vec{E}_{(n+1/2)}=\vec{E}_{(n)}+\frac{\Delta t}{2}\left(c^2\mathbf{\nabla}\times \vec{B}_{(n)}-\frac{\vec{J}_{(n)}}{\epsilon_0}\right)
\end{equation}
from this the magnetic field is given by
\begin{equation}
\vec{B}_{(n+1/2)}=\vec{B}_{(n)}-\frac{\Delta t}{2}\left(c^2\mathbf{\nabla}\times \vec{E}_{(n+1/2)}\right)
\end{equation}
(at which point the particle pusher, detailed below, updates the current to $\vec{J}_{(n+1)}$)\\
at which point we need to update the current to $\vec{J}_{(n+1)}$ in order to proceed finding the fields at time step $n+1$. This is done using the particle pusher. We update the position of each particle 
\begin{equation}
\vec{x}_{(n+1/2)}=\vec{x}_{(n)}+\frac{\Delta t}{2}\vec{v}_{(n)}
\end{equation}
from which we also obtain the intermediate velocity $\vec{v}_{(n)}$ [correct?]. Using the Lorentz force law we then compute the force $\vec{F}_{(n)}=\Delta p/\Delta t$ which gives the momentum at $n+1$ as
\begin{equation}
\vec{p}_{(n+1)}=\vec{p}_{(n)}+q\Delta t\left[\vec{E}_{(n+1/2)}\left(\vec{x}_{(n+1/2)}\right)+\vec{x}_{(n+1/2)}\times\vec{B}_{(n+1/2)}\left(\vec{x}_{(n+1/2)}\right) \right]
\end{equation}
where, the electric fields are extrapolated (?) to the intermediate point $n+1/2$. Then, using $\vec{p}=\gamma m \vec{v}$, we can find the velocity at $n+1$, from which we then have the current $\vec{J}_{(n+1)}$. We then reverse the order of computing such that the magnetic field is calculated prior to the electric field,
\begin{align}
&\vec{B}_{(n+1)}=\vec{B}_{(n+1/2)}-\frac{\Delta t}{2}\left(c^2\mathbf{\nabla}\times \vec{E}_{(n+1/2)}\right)\\
&\vec{E}_{(n+1)}=\vec{E}_{(n+1/2)}+\frac{\Delta t}{2}\left(c^2\mathbf{\nabla}\times \vec{B}_{(n+1)}-\frac{\vec{J}_{(n+1)}}{\epsilon_0}\right)
\end{align}
Using these fields when then calculate the new particles positions $\vec{x}_{(n)}$, we "push" the particles, thus completing the iteration step.

\section{EPOCH}
%The simulations presented in this thesis are generated using the open-source plasma physics PIC simulation code \textsc{EPOCH},
The Extensible PIC Open Collaboration project (EPOCH) is an advance relativistic electromagnetic PIC code developed at the University of Warwick by XXX et al. [ref.user-manual]. EPOCH is now maintained and developed through the Collaborative Computational Project in Plasma Physics (CCP-Plasma), from which access to the code is granted to non-profit research laboratories and Universities [CCP website]. 
%Simulations  using EPOCH simply require users to specify the parameters and initial conditions of the simulations without the need to interact with the underlying PIC code.
The underlying code is written in Fortran and allows for simulations to be run on multiple parallel processors via MPI; this enables time-consuming simulations to be run on remote computing clusters. The core PIC code in EPOCH is based upon the field update and particle push algorithms of the Plasma Simulation Code (PSC) written by H. Ruhl []. This follows closely the standard PIC method outline in section \ref{sec:Particle-in-Cell Codes}. The main difference being in how the FTDT method is implemented and the inclusion of additional functionality to allow for more advanced features such as collisions, ionisation and quantum electrodynamic radiation to be simulated [epoch manual].  \\
\indent EPOCH is highly user-friendly; setting up simulations simply requires users to specify the parameters and initial conditions of the simulations without the need to interact with the underlying PIC code. Likewise, analysing and visualising data from a simulations is made easier through file-compatibility with Python, Matlab, IDL and VisIt, the details of which will be covered in this section.
\subsection{Input deck}
Once EPOCH has been downloaded and compiled the so-called input deck is essentially EPOCH's user interface. This is a file in which users specify the details of a simulations and it is this file that gets read by EPOCH and passed onto the core PIC algorithm. The input deck consists of blocks which define parameters for different features of the simulation. \\
\textbf{Explain control block first}, and what the restart does.
\begin{verbatim}
begin:control
  dlb_threshold = 0.5
  restart_snapshot=restartXXXX.sdf
  t_end = end_time
  nx = nint(length / cell_length)
  ny = nint((half_width * 2) / cell_width)
  npart = part_per_cell * nx * ny
  stdout_frequency = 50
  use_random_seed = T
end:control
\end{verbatim}
This specifies the grid that the simulations is to run on. We then populate this grid with plasma particles.
\textbf{Species block}, with explanation about analytical density distributions for plasma, and specify ppc.\\
The control and species blocks together define the resolution of the simulation. When setting up the resolution of the grid one has to make sure that the grid is sufficiently fine such that the smallest features of our physical system are resolved. This is to ensure that the simulation accurately models the physical system it is meant to represent, to the extent that missing small scale phenomena might alter the large scale outcome of the simulation. A finer grid however requires more macroparticles to fully populate the grid, which inevitably extents the computational time. In addition the time step $\Delta t$ aneeds to be suitably decreased as well. This is because of the so-called Courant-Friedrichs-Lewy (CFL) condition.  Any simulation introduces uncertainties in the final outcome due to the finite resolution. We need to make sure that the uncertainties introduces during each iteration do not build up and grow unbounded. \\
\\
\textbf{edriver} with analytical distribution, and laser, followed by boundaries
\begin{center}
\begin{verbatim}
begin:boundaries
  bc_x_min = simple_laser
  bc_x_max = simple_outflow
  bc_y_min = simple_outflow
  bc_y_max = simple_outflow
end:boundaries
\end{verbatim}
\end{center}
\textbf{output block} and the sdf file visualisation with VisIT

%Parameters and initial conditions are defined using an \textit{input.deck} file.

%$$n_b = \frac{1}{{\sigma_x\sigma_y^2 (2\pi)^{3/2 }  }}e^{{{ - \left( {x - x_0 } \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {x - x_0 } \right)^2 } {2\sigma_x ^2 }}} \right. \kern-\nulldelimiterspace} {2\sigma_x^2 }}}e^{{{ - \left( {y - y_0} \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {y- y_0 } \right)^2 } {2\sigma_y ^2 }}} \right. \kern-\nulldelimiterspace} {2\sigma_y ^2 }}}e^{{{ - \left( {y - y_0 } \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {y - y_0 } \right)^2 } {2\sigma ^2 }}} \right. \kern-\nulldelimiterspace} {2\sigma_y ^2 }}}$$
\subsection{Non-analytical bunch initialisation}
\begin{itemize}
\item Issue: restart not possible with laser. 
\item EPOCH allows for a user to manually override particle-parameter distributions defined in the input deck, in which all functions must be defined analytically. By overriding this so-called autoloader, which takes the analytical distributions in the input deck and distributes the macro particles accordingly, this manual approach allows for the initialisation of a bunch with non-analytical density and momentum distributions. 
\item Furthermore, even if the density distribution were to be easily described analytically, this method offers the advantage that it also overrides the maxwellian velocity distribution that epoch assigns to each bunch of particles in the input deck. This is fine for an initial bunch in thermal equilibrium, but as soon as plasma interaction occurs the velocity distribution of the electrons in the bunch is noticeably non-maxwellian
\item VisIt - export data from .sdf file, convert to -csv, read with ic module when compiling epoch.
\item (show below, it is possible to have a laser appear before the bunch at some time t, but the parameters of this laser could not be changed so testing several differnt laser intensities, distances etc. would take far too long if the bunch was forced to propagate 20cm each time before the laser was ramped up )
\end{itemize}



\clearpage
\begin{verbatim}
ExportDBAtts = ExportDBAttributes()
ExportDBAtts.allTimes = 0
ExportDBAtts.dirname = "/Users/oscarjakobsson/Documents/epoch-4.14.4/epoch2d"
ExportDBAtts.filename = "test"
ExportDBAtts.timeStateFormat = "_%04d"
ExportDBAtts.db_type = "Xmdv"
ExportDBAtts.db_type_fullname = "Xmdv_1.0"
ExportDBAtts.variables = ("Particles/Ek/edriver", "Particles/Weight/edriver")
ExportDBAtts.writeUsingGroups = 0
ExportDBAtts.groupSize = 48
ExportDBAtts.opts.types = (0)
ExportDBAtts.opts.help = ""
ExportDatabase(ExportDBAtts)
ExportDBAtts = ExportDBAttributes()
ExportDBAtts.allTimes = 0
ExportDBAtts.dirname = "/Users/oscarjakobsson/Documents/epoch-4.14.4/epoch2d"
ExportDBAtts.filename = "test"
ExportDBAtts.timeStateFormat = "_%04d"
ExportDBAtts.db_type = "Xmdv"
ExportDBAtts.db_type_fullname = "Xmdv_1.0"
ExportDBAtts.variables = ("Particles/Ek/edriver", "Particles/Weight/edriver")
ExportDBAtts.writeUsingGroups = 0
ExportDBAtts.groupSize = 48
ExportDBAtts.opts.types = (0)
ExportDBAtts.opts.help = ""
ExportDatabase(ExportDBAtts)

\end{verbatim}


\section{Notes.}
Meeting Guoxing:
\begin{itemize}
\item We will change $\sigma_{x,y}$, in simulation from $\sigma_{x,y}=0.3 \mu m ~\to~5-10 \mu m$ because the $0.3\mu m$ EuPRAXIA beam parameter gives to high beam density $n_b$, which means that we can't have $n_b\sim n_p$ because the plasma density would have to be too high. We should aim for $n_p\sim 10^{17}-10^{18}\sim n_b$ (standard L/PWFA) parameters. 
EuPRAXIA wants $\sigma_{x,y}$ small because small bunches gives more coherent radiation in undulators. One could expand the beam by letting it propagate freely (expand due to space charge) a distance before reaching the beam dump. 
\item $\text{Run simulations with uniform plasma density for }\left\{\begin{aligned}
&n_p\sim 0.1 n_b \quad &&\text{Non-linear}\\
&n_p\sim  n_b\quad &&\text{Quasi-linear}\\
&n_p\sim 10 n_b\quad &&\text{Linear}
\end{aligned}\right.$ \\
\item Use $\Delta E/E=0.01$ and bunch charge $30~$pC ($5~$fs).\\
\item Estimate necessary simulation propagation length by saturation length using wave-breaking electric field gradient 
$$L_{\text{sat}}\approx \frac{T_0}{eE_{wb}}=\frac{T_0}{e}\frac{e}{m_e c\omega_p}=\frac{T_0}{m_e c}\sqrt{\frac{m_e e\epsilon_0}{e^2n_b}} $$ 
\item Project outline:
\begin{itemize}
\item Uniform plasma with varying $n_b\sim n_p$
\item Vary plasma density profile
\item Test laser to dump head of beam
\item Run simulations for real FlashForward parameters and not the idealized EuPRAXIA parameters.
\end{itemize}
\item  100pC $$n_b=\frac{N_p}{(2\pi)^{3/2} \sigma_y^2\sigma_x}=\frac{6.25\times 10^{8}}{(2\pi)^{3/2} (5\times 10^{-6})^3}\approx 3.2\times 10^{23}~ \text{m}^{-3} $$
$$\Rightarrow ~~eE_{\text{wb}}=\left\{\begin{aligned}
&17 ~\text{GeV/m }&& n_p=0.1 n_b \\
&54 ~\text{GeV/m} && n_p=n_b\\
&172 ~\text{GeV/m} &&n_p=10 n_b
\end{aligned}\right.\quad\Rightarrow ~~L_{sat}(1 ~\text{GeV})=\left\{\begin{aligned}
&5.8 ~\text{cm}&& n_p=0.1 n_b \\
&1.9 ~\text{cm} && n_p=n_b\\
&0.6 ~\text{cm} &&n_p=10 n_b
\end{aligned}\right.$$
$$1 ~\text{GeV beam} ~~\Rightarrow ~~ L_{sat}\sim 2 ~\text{cm}=2*10^4 \mu \text{m} $$
\item  30pC $$n_b=\frac{N_p}{(2\pi)^{3/2} \sigma_y^2\sigma_x}=\frac{1.87\times 10^{8}}{(2\pi)^{3/2} (5\times 10^{-6})^3}\approx 9.5\times 10^{22}~ \text{m}^{-3} $$
$$\Rightarrow ~~eE_{\text{wb}}=\left\{\begin{aligned}
&9.4 ~\text{GeV/m }&& n_p=0.1 n_b \\
&30 ~\text{GeV/m} && n_p=n_b\\
&94 ~\text{GeV/m} &&n_p=10 n_b
\end{aligned}\right.\quad\Rightarrow ~~L_{sat}(1 ~\text{GeV})=\left\{\begin{aligned}
&10.7 ~\text{cm}&& n_p=0.1 n_b \\
&3.4 ~\text{cm} && n_p=n_b\\
&1.1 ~\text{cm} &&n_p=10 n_b
\end{aligned}\right.$$
$$1 ~\text{GeV beam} ~~\Rightarrow ~~ L_{sat}\sim 3.4 ~\text{cm}=3.4*10^4 \mu \text{m} $$


\end{itemize}